{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dennis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PreProcessTweets' object has no attribute '_processTweet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ff2a79dde475>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mtrainingData\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/dennis/Desktop/sanders-twitter-0.2/tweetDataFile.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtweetProcessor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPreProcessTweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mppTrainingData\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtweetProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessTweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mppTestData\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtweetProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessTweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-ff2a79dde475>\u001b[0m in \u001b[0;36mprocessTweets\u001b[0;34m(self, list_of_tweets)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprocessedTweets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtweets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_tweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mprocessedTweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_processTweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprocessedTweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PreProcessTweets' object has no attribute '_processTweet'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation \n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "class PreProcessTweets:\n",
    "    def __init__(self):\n",
    "        nltk.download('stopwords')\n",
    "        self._stopwords=set(stopwords.words('english')+list(punctuation)+['AT_USER','URL'])\n",
    "\n",
    "    def processTweets(self,list_of_tweets):\n",
    "        #Create a dictionary with the key text and label\n",
    "        processedTweets=[]\n",
    "        for tweet in list_of_tweets:\n",
    "            processedTweets.append((self._processTweet(tweet[\"text\"]),tweet[\"label\"]))\n",
    "        return processedTweets\n",
    "        \n",
    "    def processTweet(self,tweet):\n",
    "        tweet=tweet.lower()\n",
    "        tweet=re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "        tweet=re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "        tweet=re.sub(r'#([^\\s]+)',r'\\1',tweet)\n",
    "        tweet=word_tokenize(tweet)\n",
    "        return [word for word in tweet if word not in self._stopwords]\n",
    "trainingData= \"/home/dennis/Desktop/sanders-twitter-0.2/tweetDataFile.csv\" \n",
    "tweetProcessor=PreProcessTweets()\n",
    "ppTrainingData=tweetProcessor.processTweets(trainingData)\n",
    "ppTestData=tweetProcessor.processTweets(testData)\n",
    "\n",
    "#Naive Bayes Classifier\n",
    "\n",
    "#First build a vocabulary\n",
    "def buildVocabulary(ppTrainingData):\n",
    "    all_word=[]\n",
    "    for (words,sentiment) in ppTrainingData:\n",
    "        all_words.extend(words)\n",
    "    \"\"\"This will create a list in which all the words in the tweet are\n",
    "    present.\"\"\"\n",
    "    wordlist=nltk.FreqDist(all_words)\n",
    "    word_features=wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "def extract_features(tweet):\n",
    "    tweet_words=set(tweet)\n",
    "    features={}\n",
    "    for word in word_features:\n",
    "        features['contain(%s)' %word]=(word in tweet_words)\n",
    "    return features\n",
    "\n",
    "word_features = buildVocabulary(ppTrainingData)\n",
    "trainingFeatures=nltk.classify.apply_features(extract_features,ppTrainingData)\n",
    "NBayesClassifier=nltk.NaiveBayesClassifier.train(trainingFeatures)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
